{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression example\n",
    "\n",
    "### I will demonstrate how the linear regressor works using a randomly generated dataset\n",
    "\n",
    "January 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random dataset to work with<br>\n",
    "**The goal is to reproduce the input coefficients and intercept**\n",
    "<br>Note - x contains the x0 column or values 1\n",
    "<br><br> ** training set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000\n",
    "\n",
    "coefficients = [6.2,-1.4,2.1,-3,11,-8]\n",
    "x = np.ones((size,len(coefficients)))\n",
    "for i in range(1,len(coefficients)):\n",
    "    x[:,i]=np.random.rand(size)\n",
    "y = (x*coefficients).sum(axis=1) + np.random.normal(size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x = np.ones((size,len(coefficients)))\n",
    "for i in range(1,len(coefficients)):\n",
    "    t_x[:,i]=np.random.rand(size)\n",
    "t_y = (t_x*coefficients).sum(axis=1) + np.random.normal(size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regressor \n",
    "\n",
    "<br>\n",
    "Note: intercept is the first term of the coefficients vector \n",
    "<br>\n",
    "Note 2: data is not scaled. Scaled results will converge faster but the coefficients will be different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_regressor import LinearRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** normal equation**\n",
    "<br>other input parameters are ignored for the normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.24591401, -1.28621158,  2.0903117 , -3.13137998, 10.98638789,\n",
       "       -8.01492882])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = Linear_regressor()\n",
    "lr.fit(x,y,method='normal')\n",
    "lr.coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coefficients are not precise because of small variations due to the noise added (above)<br>\n",
    "predictions are very close to actual values (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: 0.04499974026170541 1.0066648232304667\n"
     ]
    }
   ],
   "source": [
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " also works if x0 column is omitted (added by default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.24591401, -1.28621158,  2.0903117 , -3.13137998, 10.98638789,\n",
       "       -8.01492882])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x[:,1:],y,method='normal')\n",
    "lr.coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this, however, will fail, because it ignores the existence of an intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98865848,  4.26941362, -0.76978368, 13.40170904, -5.5292853 ])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x[:,1:],y,method='normal',add_x0=False)\n",
    "lr.coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also works with pandas DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.24591401, -1.28621158,  2.0903117 , -3.13137998, 10.98638789,\n",
       "        -8.01492882]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(pd.DataFrame(x),pd.DataFrame(y),method='normal')\n",
    "lr.coeff.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** batch gradient descent **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.24591364, -1.28621144,  2.09031183, -3.13137983, 10.98638804,\n",
       "       -8.01492866])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = Linear_regressor()\n",
    "lr.fit(x,y,method='batch',learning_rate=0.001,epochs=1000)\n",
    "lr.coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: 0.044999732675320064 1.0066648243722716\n"
     ]
    }
   ],
   "source": [
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate is automatically correcting itself if diverging (notice the number of epochs for correction to be effective is important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough epochs.. diverging\n",
      "[110792.39463514  56700.38493855  56470.63592843  57577.43638834\n",
      "  57387.94263325  58393.89372375]\n",
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: 256331.64110505104 36691.69530708807\n"
     ]
    }
   ],
   "source": [
    "lr = Linear_regressor()\n",
    "lr.fit(x,y,method='batch',learning_rate=10,epochs=10)\n",
    "print('not enough epochs.. diverging')\n",
    "print(lr.coeff)\n",
    "\n",
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divergence is self corrected\n",
      "[ 6.24591322 -1.28621128  2.09031197 -3.13137967 10.98638821 -8.01492849]\n",
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: 0.04499972432868693 1.0066648256288628\n"
     ]
    }
   ],
   "source": [
    "lr = Linear_regressor()\n",
    "lr.fit(x,y,method='batch',learning_rate=10,epochs=1000)\n",
    "print('divergence is self corrected')\n",
    "print(lr.coeff)\n",
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate is automatically increased if improvement is too shallow\n",
    "<br> tolerance is the percentage improvement cutoff to increase learning rate\n",
    "<br> default is 1% (i.e. if the improvement to the cost function is less than 1%, increase learning rate)\n",
    "<br> note: the increase and decrease of the learning rate (see above) will self regulate until convergence is achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate too slow\n",
      "[1.29574479e-05 6.22760153e-06 6.68182566e-06 5.94412380e-06\n",
      " 8.19604885e-06 5.29657516e-06]\n",
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: -6.4331154991408654 4.099024613763339\n"
     ]
    }
   ],
   "source": [
    "lr = Linear_regressor()\n",
    "lr.fit(x,y,method='batch',learning_rate=0.000000001,epochs=1000,tolerance=0)\n",
    "print('learning rate too slow')\n",
    "print(lr.coeff)\n",
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slow learning rate self corrected\n",
      "[ 6.24591342 -1.28621136  2.0903119  -3.13137975 10.98638813 -8.01492857]\n",
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: 0.044999728444160295 1.0066648250093386\n"
     ]
    }
   ],
   "source": [
    "lr = Linear_regressor()\n",
    "lr.fit(x,y,method='batch',learning_rate=0.000000001,epochs=1000,tolerance=0.01)\n",
    "print('slow learning rate self corrected')\n",
    "print(lr.coeff)\n",
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting coefficients can be provided to continue improving (rather than start from the initiated coefficients of 0)\n",
    "<br> Notice that the first 100 epochs are not enough, but then the second pass of 100 epochs provide a much better model (not the same as running 200 to begin with, because of the self correcting learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough epochs\n",
      "[1.57337471 0.73551515 0.81492666 0.68194538 1.06939357 0.56595778]\n",
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: -2.8977576979298245 4.0135220699145435\n"
     ]
    }
   ],
   "source": [
    "lr = Linear_regressor()\n",
    "lr.fit(x,y,method='batch',epochs=100)\n",
    "print('not enough epochs')\n",
    "print(lr.coeff)\n",
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue from the same point\n",
      "[ 3.099861    1.08796721  1.65599573  0.66719538  3.34485373 -0.19194288]\n",
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: -0.01717568318022522 3.4625878062029667\n"
     ]
    }
   ],
   "source": [
    "lr.fit(x,y,method='batch',epochs=100,starting_coeff=lr.coeff)\n",
    "print('continue from the same point')\n",
    "print(lr.coeff)\n",
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.37520379  0.82634999  1.828507    0.09358271  4.75337794 -1.42320025]\n",
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: -0.001866339189034683 2.939901509232615\n"
     ]
    }
   ],
   "source": [
    "lr = Linear_regressor()\n",
    "lr.fit(x,y,method='batch',epochs=200)\n",
    "print(lr.coeff)\n",
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** mini-batch gradient descent **\n",
    "<br>Note: for stochastic set bin_size to 1\n",
    "<br>Note 2: bin_size=dataset size will **not** produce the same results as using batch, because stochastic gradient descent doesn't use the self correcting learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.68928159, -1.00478809,  2.29575836, -2.87378283, 10.96070908,\n",
       "       -7.635959  ])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = Linear_regressor()\n",
    "lr.fit(x,y,method='stochastic',learning_rate=0.001,epochs=2000,bin_size=20)\n",
    "lr.coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: 0.04888470983294069 1.0213740914946345\n"
     ]
    }
   ],
   "source": [
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller bins tend to provide faster convergence (notice that the number of epochs is halves, but the predictions are comparable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.1971739 , -1.25876523,  2.17759276, -3.12043281, 10.97782032,\n",
       "       -7.99787993])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = Linear_regressor()\n",
    "lr.fit(x,y,method='stochastic',learning_rate=0.001,epochs=1000,bin_size=1)\n",
    "lr.coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: 0.06662657879586524 1.0071728611797406\n"
     ]
    }
   ],
   "source": [
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal equation works best, but if gradient descent is still wanted, start with stochastic (bin_size=1) which is faster, then use the same coefficients as input for batch, where the gradients are more precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.24589382, -1.28620412,  2.0903186 , -3.1313721 , 10.98639612,\n",
       "       -8.01492029])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = Linear_regressor()\n",
    "lr.fit(x,y,method='stochastic',learning_rate=0.001,epochs=800,bin_size=1)\n",
    "lr.fit(x,y,method='batch',learning_rate=0.001,epochs=200,starting_coeff=lr.coeff)\n",
    "lr.coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y mean and std:   6.478738468499331 4.119476341055892\n",
      "h-y mean and std: 0.044999335538243625 1.0066648841413905\n"
     ]
    }
   ],
   "source": [
    "print(\"y mean and std:  \",y.mean(),y.std())\n",
    "print(\"h-y mean and std:\",(lr.predict(t_x)-t_y).mean(),(lr.predict(t_x)-t_y).std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
