## Glossary

An incomplete list of terms related to Data Science and Machine Learning.  
  
Note: these descriptions are written in a way that helps me understand and remember them better, and I apologize if anyone finds them unhelpful.
<br>
<br>

| Term  | Brief description | Link |
| --- | --- | --- |
| AdaBoost | An ensemble learning method where several weak learners are trained sequentially, and the training samples are weighted based on whether they were correctly classified or not, in the previous iteration. | - |
| AdaGrad | An algorithm for SGD where the learning rate is adapted based on the frequency of features. It is a good algorithm for sparse data (e.g. bag of words). Related: RMSprop. | [link](http://ruder.io/optimizing-gradient-descent/index.html#adagrad) |
| Adversarial learning | A technique used, mostly for malicious motivations, to fool machine learning tools (*e.g.* using special character to fool simple spam classifiers. | [rifle-turtle](https://www.theverge.com/2017/11/2/16597276/google-ai-image-attacks-adversarial-turtle-rifle-3d-printed) |
| Autodiff | A computational algorithm to find the derivative of a function. Note, that this is not a numerical ('trial and error') not a symbolic (hard coding the derivative by a user) solution. | [Intuition tutorial](https://www.youtube.com/watch?v=twTIGuVhKbQ) |
| Container technology | An application bundled together with all its dependencies, libraries, binaries, and configuration files needed to run it - to avoid problems when migrating between different computers (or similar). | - |
| DNA storage | A method for storing digital data encoded into DNA (using the standard nucleotides). The achieved compression is enormous, but reading from the DNA is a slow process. | - |
| Dropout / Dropconnect | Regularization methods for training ANNs. In Dropout, randomly selected neurons have their activation output set to zero, in each training sample. In dropconnect, randomly selected individual weights are set to zero, in each training sample. The weights have to be adjusted at the end of the training to account for this process. | [Stack Exchange](https://stats.stackexchange.com/questions/201569/difference-between-dropout-and-dropconnect/201891) |
| Eligibility trace | | - |
| ETL/ELT | Strategies for dealing with big data and storing it in a data warehouse. Extract, Transform, Load - you extract the data from a source, transform it (e.g. make it tabular) and then load it (store it in the warehouse). Extract, Load, Transform - you extract the data and store it as is, then use technology for the transformation (e.g. Hadoop). | - |
| Label powerset | A multilabel classification solution (not to be confused with multiclass label classificiation) where a classifier is generated for each label combination in the dataset (up to the total combinatorial possibilities in n-labels). The advantage is that it considers correlations between the labels. | [link](https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff) |
| Model zoo | A collection of pre-trained models (typically networks) for specific inferences. | - |
| Multi-level modeling | A modeling scheme with hierarchy. Different models are put together, where each focuses on features at different hierarchies (e.g. nested features). For example, a model for predicting student's performance based on students features, then school features, then city features, then country features. | - |
| R-CNN | Region-CNN (and related), a group of approaches for object detection and segementation. In R-CNN regions of interest (ROIs) are provided by an external, non-DL method, and the ROIs are given to the CNN. In Fast-RCNN the ROIs are applied to the convolved intermediate rather than the raw image. In Faster-RCNN a branched-out FC-layer finds the ROIs by itself. In Mask-RCNN stacked convolutions also do sematic segmentation. | [video](https://www.youtube.com/watch?v=nDPWywWRIRo) |
| Shallow learning | Simplistic definition: ML methods that rely on user input (such as feature selection and manipulation), as opposed to deep learning, where the user only defines the architecture, and the deep learner finds the patterns and importances in all the raw data. | - |
| SSD/YOLO | Single Shot Detector / You Only Look Once. Methods for object detection and segmentation; the image is divided by a grid, and on each segment several pre-defined bounding boxes (of various aspect-ratios) are run through a CNN with FC layers. The output is a high-rank tensor that provides classification and bounding box corrections for each object detected. | - |
| Survival analysis | A tool used to investigate the time it takes for an event of interest to occur (*e.g.* death of an organism).  | - |
| t-SNA | Student t-distribution Stochastic Neighbor Embedding: a method for dimentionality reduction. In brief and simple terms, it generates a similarity matrix using a Gaussian distribution of the input data, then it randomly scatters the data into the lower dimension and slowly tries to move the samples in the lower dimension using a t-distribution, until it resembles the input similarity matrix. Its advantage over several other dimentionality reduction techniques is that it generally also conserves local structure (in addition to global structure). | [video1](https://www.youtube.com/watch?v=NEaUSP4YerM) [video2](https://www.youtube.com/watch?v=ohQXphVSEQM) |
| Whitening | A preprocessing method where the data is transformed such that there is no correlation between the features and all features have a variance of 1. Whiteningn can be done using PCA, and can improve the learning of a ML algorithm. | [link](http://mccormickml.com/2014/06/03/deep-learning-tutorial-pca-and-whitening/) | 
