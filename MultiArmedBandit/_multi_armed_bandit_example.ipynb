{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various algorithms that optimize the problem by employing explore/exploit strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Namely**:\n",
    "\n",
    " * Random (for test purposes)\n",
    " * $\\varepsilon$-Greedy\n",
    " * Decaying $\\varepsilon$-Greedy (2 decay schemes)\n",
    " * Optimistic initial value\n",
    " * Upper confidence bound (UCB)\n",
    " * Thompson sampling (Gaussian, binary, and an experimental hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** these algorithms were developed for bound output [0,1] (or at least normal distribution with $\\mu\\in$~ (0-1] and $\\sigma^2\\approx$1) and I will demonsrate below where some of them underperform at certain conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 2:** all demonstrations below are based on random data, and a therefore prone to stochasticity (if rerun using different seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from multi_armed import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aka the slot machines the player tries to choose from**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bandit with a random output normally distributed\n",
    "# Note: output is pregenerated so that the different algorithms can be compared using the same win/loss values\n",
    "class Bandit():\n",
    "    \n",
    "    def __init__(self,size,E=0,s=1,variance_on_mean=False):\n",
    "        # initial mean is chosen at random\n",
    "        if (variance_on_mean):\n",
    "            self.mean = np.random.normal(E,s)\n",
    "        else:\n",
    "            self.mean = np.random.normal(E,1)            \n",
    "        # pre-generate all the data *+1 because counts start from 1 (see multi_armed.py)\n",
    "        # always affected by variance\n",
    "        self.data = np.random.normal(self.mean,s,size=size+1)\n",
    "    \n",
    "    def play(self,i):\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bandit with a binary win/lose output\n",
    "class BinaryBandit():\n",
    "    \n",
    "    def __init__(self,size,mean):\n",
    "        self.mean = mean\n",
    "        self.data = np.random.rand(size+1)\n",
    "    \n",
    "    def play(self,i):\n",
    "        output = self.data[i]\n",
    "        # return 0 or 1\n",
    "        return int(output<self.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate datasets to test players on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays = 10000\n",
    "no_of_bandits = 4\n",
    "\n",
    "# numerical dataset\n",
    "n_bandits = []\n",
    "for i in range(no_of_bandits):\n",
    "    # normal distribution (e=1, s=1)\n",
    "    n_bandits.append(Bandit(plays,1))\n",
    "    \n",
    "# binary dataset\n",
    "b_bandits = []\n",
    "for i in range(no_of_bandits):\n",
    "    b_bandits.append(BinaryBandit(plays,np.random.rand()))\n",
    "    \n",
    "# test player method\n",
    "def test_player(player,bandits,plays):\n",
    "    player.start(bandits)\n",
    "    for i in range(plays):\n",
    "        # choose bandit to play\n",
    "        bandit = player.choose()\n",
    "        # play bandit\n",
    "        outcome = bandits[bandit].play(i)\n",
    "        # update player\n",
    "        player.update(bandit,outcome)\n",
    "    return player.profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean maximum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19379"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "best = n_bandits[np.argmax([b.mean for b in n_bandits])]\n",
    "for i in range(plays):\n",
    "    sum+=best.play(i)\n",
    "int(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random player**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12763"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = RandomPlayer()\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon greedy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19018"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = EpsilonGreedy(0.01)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18075"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = EpsilonGreedy(0.2)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16070"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = EpsilonGreedy(0.0001)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: value of epsilon needs to be experimented for best results\n",
    "<br><br>\n",
    "Otherwise, use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dacaying epsilon greedy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon value decays over time, assuming convergence\n",
    "\n",
    "**Scheme 1:** Epsilon decays as b/N where N is the number of plays so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16067"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = DecayingEpsilonGreedy(b=1)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/N is sometimes too fast, and the exploration is too short. User should provide b > number of bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19307"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = DecayingEpsilonGreedy(b=10)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not too big either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17188"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = DecayingEpsilonGreedy(b=1000)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scheme 2:** Epsilon decays by a fixed fraction (the decay rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19366"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = DecayingEpsilonGreedy(decay=0.95)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, performance depends on the rate chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16068"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = DecayingEpsilonGreedy(decay=0.5)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18747"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = DecayingEpsilonGreedy(decay=0.999)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimistic initial value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19356"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = OptimsiticInitialValue(10)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial value must indeed be optimistic (higher than the best bandit's mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16068"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = OptimsiticInitialValue(0.2)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too high values will take longer to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16890"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = OptimsiticInitialValue(1000)\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upper confidence bound**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19306"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = UCB()\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: UCB can take a variance parameter, see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thompson sampling (Gaussian)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19343"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = ThompsonSampling()\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: TS can also take a variance parameter, see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thompson sampling (Binary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, 0 for any loss and 1 for any profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19133"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = BinaryThompsonSampling()\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thompson sampling (Experimental hybrid)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19267"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = SelfRegulatingThompsonSampling()\n",
    "players.append(player)\n",
    "int(test_player(player,n_bandits,plays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean maximum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8809"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "best = b_bandits[np.argmax([b.mean for b in b_bandits])]\n",
    "for i in range(plays):\n",
    "    sum+=best.play(i)\n",
    "int(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All players:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomPlayer\n",
      "\t 6764\n",
      "EpsilonGreedy epsilon: 0.01\n",
      "\t 8580\n",
      "EpsilonGreedy epsilon: 0.2\n",
      "\t 8389\n",
      "EpsilonGreedy epsilon: 0.0001\n",
      "\t 7789\n",
      "DecayingEpsilonGreedyb: 1\n",
      "\t 7791\n",
      "DecayingEpsilonGreedyb: 10\n",
      "\t 8796\n",
      "DecayingEpsilonGreedyb: 1000\n",
      "\t 8122\n",
      "DecayingEpsilonGreedydecay: 0.95\n",
      "\t 7790\n",
      "DecayingEpsilonGreedydecay: 0.5\n",
      "\t 7790\n",
      "DecayingEpsilonGreedydecay: 0.999\n",
      "\t 7790\n",
      "OptimisticInitialValue:10\n",
      "\t 8778\n",
      "OptimisticInitialValue:0.2\n",
      "\t 8809\n",
      "OptimisticInitialValue:1000\n",
      "\t 7385\n",
      "UpperConfidenceBound\n",
      "\t 8663\n",
      "GaussianThompsonSampling\n",
      "\t 8734\n",
      "BinaryThompsonSampling\n",
      "\t 8794\n",
      "SelfRegulatingThompsonSampling(Experimental)\n",
      "\t 8793\n"
     ]
    }
   ],
   "source": [
    "for player in players:\n",
    "    print(player)\n",
    "    print('\\t',int(test_player(player,b_bandits,plays)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general trend is similar to the numerical bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific cases where some players fail will be explored, and possible solutions will be demonstrated.\n",
    "<br>\n",
    "Note: the experimental `SelfRegulatingThompsonSampling` has no hyper-parameters, and cannot be tweaked to succeed, but overall performs decently in all cases with no intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = []\n",
    "for i in range(no_of_bandits):\n",
    "    # note that the means are not affected by the variance\n",
    "    # (this is important for the demonstration, because otherwis there is one bandit that is obviously\n",
    "        # superior, and little sophistication is required to spot it)\n",
    "    hv.append(Bandit(plays,1,20))\n",
    "\n",
    "# subset of players to test\n",
    "players = []\n",
    "players.append(DecayingEpsilonGreedy(b=1))\n",
    "players.append(DecayingEpsilonGreedy(decay=0.95))\n",
    "players.append(OptimsiticInitialValue(10))\n",
    "players.append(UCB())\n",
    "players.append(ThompsonSampling())\n",
    "players.append(BinaryThompsonSampling())\n",
    "players.append(SelfRegulatingThompsonSampling())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecayingEpsilonGreedyb: 1\n",
      "\t 23598\n",
      "DecayingEpsilonGreedydecay: 0.95\n",
      "\t 8217\n",
      "OptimisticInitialValue:10\n",
      "\t 16550\n",
      "UpperConfidenceBound\n",
      "\t -4786\n",
      "GaussianThompsonSampling\n",
      "\t 16623\n",
      "BinaryThompsonSampling\n",
      "\t 21749\n",
      "SelfRegulatingThompsonSampling(Experimental)\n",
      "\t 20061\n"
     ]
    }
   ],
   "source": [
    "for player in players:\n",
    "    print(player)\n",
    "    print('\\t',int(test_player(player,hv,plays)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High variance breaks many of the players, because they set their heuristic assuming variance of 1\n",
    "<br>\n",
    "This can be corrected, by providing the players with estimated variance (or bounds taking the potential variance into account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecayingEpsilonGreedyb: 20\n",
      "\t 24181\n",
      "DecayingEpsilonGreedydecay: 0.8\n",
      "\t 24326\n",
      "OptimisticInitialValue:50\n",
      "\t 24567\n",
      "UpperConfidenceBound variance: 10\n",
      "\t 22022\n",
      "GaussianThompsonSampling variance: 10\n",
      "\t 21836\n",
      "BinaryThompsonSampling\n",
      "\t 20266\n",
      "SelfRegulatingThompsonSampling(Experimental)\n",
      "\t 19238\n"
     ]
    }
   ],
   "source": [
    "# corrected values\n",
    "players = []\n",
    "# high variance will require more time to converge, increase b\n",
    "players.append(DecayingEpsilonGreedy(b=20))\n",
    "# performance was ok\n",
    "players.append(DecayingEpsilonGreedy(decay=0.8))\n",
    "# high variance requires an overly optimistic value\n",
    "players.append(OptimsiticInitialValue(50))\n",
    "# provide estimate of variance\n",
    "players.append(UCB(10))\n",
    "# same\n",
    "players.append(ThompsonSampling(10))\n",
    "# performance was ok\n",
    "players.append(BinaryThompsonSampling())\n",
    "players.append(SelfRegulatingThompsonSampling())\n",
    "\n",
    "for player in players:\n",
    "    print(player)\n",
    "    print('\\t',int(test_player(player,hv,plays)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High bias** (all means are very positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecayingEpsilonGreedyb: 1\n",
      "\t 319818\n",
      "DecayingEpsilonGreedydecay: 0.95\n",
      "\t 258391\n",
      "OptimisticInitialValue:10\n",
      "\t 177943\n",
      "UpperConfidenceBound\n",
      "\t 177943\n",
      "GaussianThompsonSampling\n",
      "\t 177943\n",
      "BinaryThompsonSampling\n",
      "\t 263992\n",
      "SelfRegulatingThompsonSampling(Experimental)\n",
      "\t 319287\n"
     ]
    }
   ],
   "source": [
    "hb = []\n",
    "for i in range(no_of_bandits):\n",
    "    # note that the variance affects the means in this case\n",
    "    # If all means are high but the variance of the means is low, \n",
    "        # all bandits are nearly equal, and no strategy is needed)\n",
    "    hb.append(Bandit(plays,20,5,True))\n",
    "\n",
    "# subset of players to test\n",
    "players = []\n",
    "players.append(DecayingEpsilonGreedy(b=1))\n",
    "players.append(DecayingEpsilonGreedy(decay=0.95))\n",
    "players.append(OptimsiticInitialValue(10))\n",
    "players.append(UCB())\n",
    "players.append(ThompsonSampling())\n",
    "players.append(BinaryThompsonSampling())\n",
    "players.append(SelfRegulatingThompsonSampling())\n",
    "\n",
    "for player in players:\n",
    "    print(player)\n",
    "    print('\\t',int(test_player(player,hb,plays)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all bandits have high profit (but are quite different) many algorithms break, because they observe high outcome and assume this is unique and not universal, and no further optimization is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecayingEpsilonGreedyb: 20\n",
      "\t 319210\n",
      "DecayingEpsilonGreedydecay: 0.65\n",
      "\t 320241\n",
      "OptimisticInitialValue:50\n",
      "\t 320173\n",
      "UpperConfidenceBound variance: 20\n",
      "\t 318401\n",
      "GaussianThompsonSampling variance: 20\n",
      "\t 319162\n",
      "BinaryThompsonSampling\n",
      "\t 217103\n",
      "SelfRegulatingThompsonSampling(Experimental)\n",
      "\t 319181\n"
     ]
    }
   ],
   "source": [
    "# corrected values\n",
    "players = []\n",
    "# performce is mostly affected by the variance itself rather than the bias\n",
    "players.append(DecayingEpsilonGreedy(b=20))\n",
    "# performance was ok\n",
    "players.append(DecayingEpsilonGreedy(decay=0.65))\n",
    "# initial value must by higher than the highest reasonable outcome\n",
    "players.append(OptimsiticInitialValue(50))\n",
    "# artificially high variance could overcome the issue, but the performance will still be hindered\n",
    "players.append(UCB(20))\n",
    "# same\n",
    "players.append(ThompsonSampling(20))\n",
    "# Binary TS cannot be fixed in this case - the data must be normalized\n",
    "players.append(BinaryThompsonSampling())\n",
    "players.append(SelfRegulatingThompsonSampling())\n",
    "\n",
    "for player in players:\n",
    "    print(player)\n",
    "    print('\\t',int(test_player(player,hb,plays)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Many iterations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas most algorithms tend to converge and perform better with more iterations, the nondecaying `EpsilonGreedy` will actually underperform after some time. <br><br>\n",
    "Also, the `SelfRegulatingThompsonSampling` will underperform (see `multi_armed.py` for more details).\n",
    "<br><br> In practice though, this doesn't seem to be very apparent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpsilonGreedy epsilon: 0.01\n",
      "\t 915178\n",
      "DecayingEpsilonGreedydecay: 0.95\n",
      "\t 917779\n",
      "GaussianThompsonSampling\n",
      "\t 917610\n",
      "SelfRegulatingThompsonSampling(Experimental)\n",
      "\t 917769\n"
     ]
    }
   ],
   "source": [
    "long = []\n",
    "steps = 1000000\n",
    "\n",
    "for i in range(no_of_bandits):\n",
    "    # note that the variance affects the means in this case\n",
    "    # If all means are high but the variance of the means is low, \n",
    "        # all bandits are nearly equal, and no strategy is needed)\n",
    "    long.append(BinaryBandit(steps,np.random.rand()))\n",
    "\n",
    "# subset of players to test\n",
    "players = []\n",
    "players.append(EpsilonGreedy(0.01))\n",
    "players.append(DecayingEpsilonGreedy(decay=0.95))\n",
    "players.append(ThompsonSampling())\n",
    "players.append(SelfRegulatingThompsonSampling())\n",
    "\n",
    "for player in players:\n",
    "    print(player)\n",
    "    print('\\t',int(test_player(player,long,steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very small values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecayingEpsilonGreedyb: 1\n",
      "\t 24\n",
      "DecayingEpsilonGreedydecay: 0.95\n",
      "\t 24\n",
      "OptimisticInitialValue:10\n",
      "\t 10\n",
      "UpperConfidenceBound\n",
      "\t 8\n",
      "GaussianThompsonSampling\n",
      "\t 8\n",
      "BinaryThompsonSampling\n",
      "\t 24\n",
      "SelfRegulatingThompsonSampling(Experimental)\n",
      "\t 24\n"
     ]
    }
   ],
   "source": [
    "small = []\n",
    "for i in range(no_of_bandits):\n",
    "    # note that the variance affects the means in this case\n",
    "    # If all means are high but the variance of the means is low, \n",
    "        # all bandits are nearly equal, and no strategy is needed)\n",
    "    small.append(Bandit(plays,0.001,0.001,True))\n",
    "\n",
    "# subset of players to test\n",
    "players = []\n",
    "players.append(DecayingEpsilonGreedy(b=1))\n",
    "players.append(DecayingEpsilonGreedy(decay=0.95))\n",
    "players.append(OptimsiticInitialValue(10))\n",
    "players.append(UCB())\n",
    "players.append(ThompsonSampling())\n",
    "players.append(BinaryThompsonSampling())\n",
    "players.append(SelfRegulatingThompsonSampling())\n",
    "\n",
    "for player in players:\n",
    "    print(player)\n",
    "    print('\\t',int(test_player(player,small,plays)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small values and small variance take too long to converge for some algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecayingEpsilonGreedyb: 1\n",
      "\t 24\n",
      "DecayingEpsilonGreedydecay: 0.95\n",
      "\t 24\n",
      "OptimisticInitialValue:0.1\n",
      "\t 24\n",
      "UpperConfidenceBound variance: 0.01\n",
      "\t 22\n",
      "GaussianThompsonSampling variance: 0.01\n",
      "\t 23\n",
      "BinaryThompsonSampling\n",
      "\t 24\n",
      "SelfRegulatingThompsonSampling(Experimental)\n",
      "\t 24\n"
     ]
    }
   ],
   "source": [
    "# corrected values\n",
    "players = []\n",
    "# performance was ok\n",
    "players.append(DecayingEpsilonGreedy(b=1))\n",
    "# performance was ok\n",
    "players.append(DecayingEpsilonGreedy(decay=0.95))\n",
    "# optimistic value needs to be smaller\n",
    "players.append(OptimsiticInitialValue(0.1))\n",
    "# provide small variance\n",
    "players.append(UCB(0.01))\n",
    "# same\n",
    "players.append(ThompsonSampling(0.01))\n",
    "# performance was ok\n",
    "players.append(BinaryThompsonSampling())\n",
    "# performance was ok\n",
    "players.append(SelfRegulatingThompsonSampling())\n",
    "\n",
    "for player in players:\n",
    "    print(player)\n",
    "    print('\\t',int(test_player(player,small,plays)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Many bandits** (or few iterations compared to the number of bandits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecayingEpsilonGreedyb: 1\n",
      "\t 14597\n",
      "DecayingEpsilonGreedydecay: 0.95\n",
      "\t 24949\n",
      "OptimisticInitialValue:10\n",
      "\t 36865\n",
      "UpperConfidenceBound\n",
      "\t 37348\n",
      "GaussianThompsonSampling\n",
      "\t 37260\n",
      "BinaryThompsonSampling\n",
      "\t 32318\n",
      "SelfRegulatingThompsonSampling(Experimental)\n",
      "\t 34892\n"
     ]
    }
   ],
   "source": [
    "many = []\n",
    "many_bandits = 100\n",
    "\n",
    "for i in range(many_bandits):\n",
    "    # note that the variance affects the means in this case\n",
    "    # If all means are high but the variance of the means is low, \n",
    "        # all bandits are nearly equal, and no strategy is needed)\n",
    "    many.append(Bandit(plays,1))\n",
    "\n",
    "# subset of players to test\n",
    "players = []\n",
    "players.append(DecayingEpsilonGreedy(b=1))\n",
    "players.append(DecayingEpsilonGreedy(decay=0.95))\n",
    "players.append(OptimsiticInitialValue(10))\n",
    "players.append(UCB())\n",
    "players.append(ThompsonSampling())\n",
    "players.append(BinaryThompsonSampling())\n",
    "players.append(SelfRegulatingThompsonSampling())\n",
    "\n",
    "for player in players:\n",
    "    print(player)\n",
    "    print('\\t',int(test_player(player,many,plays)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DecayingEpsilonGreedy`s decay too fast to explore that many bandits. BinaryThompson probably does not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecayingEpsilonGreedyb: 100\n",
      "\t 36687\n",
      "DecayingEpsilonGreedydecay: 0.9\n",
      "\t 34706\n",
      "GaussianThompsonSampling\n",
      "\t 37681\n",
      "BinaryThompsonSampling\n",
      "\t 32733\n",
      "SelfRegulatingThompsonSampling(Experimental)\n",
      "\t 34291\n"
     ]
    }
   ],
   "source": [
    "# corrected values\n",
    "players = []\n",
    "# decrease decay\n",
    "players.append(DecayingEpsilonGreedy(b=100))\n",
    "# decrease decay\n",
    "players.append(DecayingEpsilonGreedy(decay=0.9))\n",
    "players.append(ThompsonSampling())\n",
    "# Nothing can be done, run more iterations if possible\n",
    "players.append(BinaryThompsonSampling())\n",
    "# performance was ok\n",
    "players.append(SelfRegulatingThompsonSampling())\n",
    "\n",
    "for player in players:\n",
    "    print(player)\n",
    "    print('\\t',int(test_player(player,many,plays)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each algorithm has cases where it underperforms, choice should be case-dependent**\n",
    "\n",
    "**As a rule of thumb though, `DecayingEpsilonGreedy` appear to perform universally well**\n",
    "The many bandits is a special case that should be known (as opposed to low/high values or variance, that is not accessible in a real scenario) and so the `DecayingEpsilonGreedy` is probably the safest choise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
