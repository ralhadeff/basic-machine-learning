{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.distributions import Normal\n",
    "from tensorflow.distributions import Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This module is not intended to run by iself\n"
     ]
    }
   ],
   "source": [
    "'''A Convolution Variational Auto Encoder'''\n",
    "\n",
    "class ConvVAE:\n",
    "    \n",
    "    def __init__(self, image_shape=(128,128,3), conv_param=(3,16,True), n_list=[256,32]):\n",
    "        # input data\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, *image_shape))\n",
    "        \n",
    "        # encoder\n",
    "        self.encoder_layers = []\n",
    "        # convolution layer\n",
    "        h = Conv2DLayer(image_shape[2],conv_param[0],conv_param[1],conv_param[2])\n",
    "        self.encoder_layers.append(h)\n",
    "        # flatten layer\n",
    "        self.encoder_layers.append(FlattenLayer())\n",
    "        # calculate number of input neurons to the FC layer\n",
    "        previous = image_shape[0]*image_shape[1]*conv_param[1]\n",
    "        if conv_param[2]:\n",
    "            previous=previous//4\n",
    "        # save for later\n",
    "        flat = previous\n",
    "        # current is the output of each layer (skip last because there is nothing after it)\n",
    "        for current in n_list[:-1]:\n",
    "            h = DenseLayer(previous,current)\n",
    "            self.encoder_layers.append(h)\n",
    "            previous = current\n",
    "        # latent features number\n",
    "        latent = n_list[-1]\n",
    "        encoder_output = DenseLayer(current,latent*2,activation='none')\n",
    "        self.encoder_layers.append(encoder_output)\n",
    "        \n",
    "        # feed forward through encoder\n",
    "        c_X = self.X\n",
    "        for layer in self.encoder_layers:\n",
    "            c_X = layer.feed_forward(c_X)\n",
    "        # c_X now holds the output of the encoder\n",
    "        # first half are the means\n",
    "        self.means = c_X[:,:latent]\n",
    "        # second half are the std; must be positive; +1e-6 for smoothing\n",
    "        self.std = tf.nn.softplus(c_X[:,latent:]) + 1e-6\n",
    "        \n",
    "        # reparameterization trick\n",
    "        normal = Normal(loc=self.means,scale=self.std)\n",
    "        self.Z = normal.sample()\n",
    "                \n",
    "        # decoder\n",
    "        self.decoder_layers = []\n",
    "        previous = latent\n",
    "        for current in reversed(n_list[:-1]):\n",
    "            h = DenseLayer(previous,current)\n",
    "            self.decoder_layers.append(h)\n",
    "            previous = current\n",
    "        decoder_output = DenseLayer(previous,flat,activation=lambda x:x)\n",
    "        self.decoder_layers.append(decoder_output)       \n",
    "        #feed forward through decoder, using the sampled 'data'\n",
    "        c_X = self.Z\n",
    "        for layer in self.decoder_layers:\n",
    "            c_X = layer.feed_forward(c_X)\n",
    "        # reshape\n",
    "        if (conv_param[2]):\n",
    "            shape = [-1,image_shape[0]//2,image_shape[0]//2,conv_param[1]]\n",
    "        else:\n",
    "            shape = [-1,image_shape[0],image_shape[0],conv_param[1]]\n",
    "        c_X = tf.reshape(c_X,shape)\n",
    "        # convolution transpose\n",
    "        self.trans_k = tf.Variable(tf.truncated_normal(\n",
    "            [conv_param[0],conv_param[0],image_shape[2],conv_param[1]],stddev=0.1))\n",
    "        if (conv_param[2]):\n",
    "            strides=(1,2,2,1)\n",
    "        else:\n",
    "            strides=(1,1,1,1)\n",
    "        c_X = tf.nn.conv2d_transpose(c_X, self.trans_k,strides=strides,padding='SAME',\n",
    "                                     output_shape=[-1,*image_shape])\n",
    "\n",
    "        # output logit\n",
    "        logits = c_X\n",
    "        # use logits for cost function below\n",
    "        neg_cross_entropy = -tf.nn.sigmoid_cross_entropy_with_logits(labels=self.X,\n",
    "                    logits=logits)\n",
    "        neg_cross_entropy = tf.reduce_sum(neg_cross_entropy, 1)\n",
    "        \n",
    "        # output\n",
    "        self.y_prob = Bernoulli(logits=logits)\n",
    "        \n",
    "        # sample from output\n",
    "        self.post_pred = self.y_prob.sample()\n",
    "        self.post_pred_probs = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        # generate 'de-novo' output\n",
    "        self.gen = tf.Variable(0)\n",
    "        Z_std = Normal(0.0,1.0).sample([self.gen,latent])\n",
    "        c_X = Z_std\n",
    "        for layer in self.decoder_layers:\n",
    "            c_X = layer.feed_forward(c_X)\n",
    "        logits = c_X\n",
    "        \n",
    "        prior_pred_dist = Bernoulli(logits=logits)\n",
    "        self.prior_pred = prior_pred_dist.sample()\n",
    "        self.prior_pred_probs = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        # manually input Z\n",
    "        self.Z_input = tf.placeholder(np.float32, shape=(None, latent))\n",
    "        c_X = self.Z_input\n",
    "        for layer in self.decoder_layers:\n",
    "            c_X = layer.feed_forward(c_X)\n",
    "        logits = c_X\n",
    "        self.manual_prior_prob = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        # cost function\n",
    "        # Kullbackâ€“Leibler divergence\n",
    "        kl = -tf.log(self.std) + 0.5*(self.std**2 + self.means**2) - 0.5\n",
    "        kl = tf.reduce_sum(kl, axis=1)\n",
    "        # ELBO\n",
    "        self.elbo = tf.reduce_sum(neg_cross_entropy - kl)\n",
    "        \n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(-self.elbo)\n",
    "            \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(self.init)\n",
    "        \n",
    "    def fit(self,X,epochs=10,batch=50):\n",
    "        n_batches = len(X) // batch\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch:',epoch+1)\n",
    "            np.random.shuffle(X)\n",
    "            cost = 0\n",
    "            for b in range(n_batches):\n",
    "                c_batch = X[b*batch:(b+1)*batch]\n",
    "                _,c, = self.session.run((self.optimizer, self.elbo),feed_dict={self.X: c_batch})\n",
    "                # accumulate cost\n",
    "                cost+=c\n",
    "            print('Cost:', cost)\n",
    "            \n",
    "    def predict(self,X,out='prob'):\n",
    "        '''\n",
    "        Pass data through encoder and decoder and retrieve reconstructed output\n",
    "            by default the probabilities are returned, user can specify 'sample' or 'both'\n",
    "        '''\n",
    "        # correct shape if needed\n",
    "        if (X.ndim==1):\n",
    "            X = X.reshape([1,-1])\n",
    "        pred,prob = self.session.run((self.post_pred,self.post_pred_probs),feed_dict={self.X:X})\n",
    "        if (out=='prob'):\n",
    "            return prob\n",
    "        elif (out=='sample'):\n",
    "            return pred\n",
    "        else:\n",
    "            return pred,prob\n",
    "\n",
    "    def generate(self,n=1,out='prob'):\n",
    "        '''\n",
    "        Generate output\n",
    "            by default the probabilities are returned, user can specify 'sample' or 'both'\n",
    "            User specifies the number of points requested \n",
    "        '''\n",
    "        pred,prob = self.session.run((self.prior_pred,self.prior_pred_probs),feed_dict={self.gen:n})\n",
    "        if (out=='prob'):\n",
    "            return prob\n",
    "        elif (out=='sample'):\n",
    "            return pred\n",
    "        else:\n",
    "            return pred,prob\n",
    "    \n",
    "    def feed(self,Z):\n",
    "        '''Generate output using provided latent-space input Z'''\n",
    "        # correct shape if needed\n",
    "        if (Z.ndim==1):\n",
    "            Z = Z.reshape([1,-1])\n",
    "        return self.session.run(self.manual_prior_prob,feed_dict={self.Z_input:Z})\n",
    "    \n",
    "    def close(self):\n",
    "        self.session.close()\n",
    "\n",
    "class DenseLayer(object):\n",
    "    '''A fully connected layer'''\n",
    "    \n",
    "    def __init__(self, n_in, n_out, activation=tf.nn.relu):\n",
    "        '''number of input and output neurons; the activation function'''\n",
    "        self.weights = tf.Variable(tf.random_normal(shape=(n_in, n_out), stddev=2/np.sqrt(n_in)))\n",
    "        self.bias = tf.Variable(tf.constant(0.0,shape=[n_out]))\n",
    "        if (activation=='none'):\n",
    "            self.activation = lambda x: x\n",
    "        else:\n",
    "            self.activation = activation\n",
    "            \n",
    "    def feed_forward(self, X):\n",
    "        '''Run input through layer and retrieve output'''\n",
    "        return self.activation(tf.matmul(X, self.weights) + self.bias)\n",
    "\n",
    "class Conv2DLayer(object):\n",
    "    '''A Conv2D layer'''\n",
    "    \n",
    "    def __init__(self,channels,k_size,n_kernels,use_pooling=True):\n",
    "        shape=[k_size,k_size,channels,n_kernels]\n",
    "        self.kernels = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "        self.bias = tf.Variable(tf.constant(0.0,shape=[n_kernels]))\n",
    "        self.use_pooling = use_pooling\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        layer = tf.nn.conv2d(input=X,filter=self.kernels,strides=[1,1,1,1],padding='SAME')\n",
    "        layer+= self.bias\n",
    "        if self.use_pooling:\n",
    "            layer = tf.nn.max_pool(layer,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "        layer = tf.nn.relu(layer)\n",
    "        return layer\n",
    "\n",
    "class FlattenLayer(object):\n",
    "    '''A flattening layer'''\n",
    "          \n",
    "    def feed_forward(self,X):\n",
    "        shape = X.shape\n",
    "        num_features = np.array(shape[1:4],dtype=int).prod()\n",
    "        layer = tf.reshape(X,[-1,num_features])\n",
    "        return layer\n",
    "        \n",
    "if (__name__ == '__main__'):\n",
    "    print(\"This module is not intended to run by iself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = ConvVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
