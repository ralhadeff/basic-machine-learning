{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.distributions import Normal\n",
    "from tensorflow.distributions import Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(object):\n",
    "    '''A fully connected layer'''\n",
    "    \n",
    "    def __init__(self, n_in, n_out, activation=tf.nn.relu):\n",
    "        self.weights = tf.Variable(tf.random_normal(shape=(n_in, n_out), stddev=2/np.sqrt(n_in)))\n",
    "        self.bias = tf.Variable(tf.constant(0.0,shape=[n_out]))\n",
    "        self.activation = activation\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        return self.activation(tf.matmul(X, self.weights) + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder:\n",
    "    def __init__(self, n_input, n_list):\n",
    "        ''''''\n",
    "        # input\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, n_input))\n",
    "        \n",
    "        # encoder\n",
    "        # build hidden layers\n",
    "        self.encoder_layers = []\n",
    "        # input of first hidden layer\n",
    "        previous = n_input\n",
    "        # current is the output of each layer (skip last because there is nothing after it)\n",
    "        for current in n_list[:-1]:\n",
    "            # hidden layer\n",
    "            h = DenseLayer(previous,current)\n",
    "            self.encoder_layers.append(h)\n",
    "            previous = current\n",
    "        # latent features number\n",
    "        latent = n_list[-1]\n",
    "        encoder_output = DenseLayer(current,latent*2,activation=lambda x:X)\n",
    "        self.encoder_layers.append(encoder_output)\n",
    "        \n",
    "        # feed forward through encoder\n",
    "        c_X = self.X\n",
    "        for layer in self.encoder_layers:\n",
    "            c_X = layer.feed_forward(c_X)\n",
    "        # c_X now holds the output of the encoder\n",
    "        self.means = c_X[:,:latent]\n",
    "        # std must be positive, 1e-6 for smoothing\n",
    "        self.std = tf.nn.softplus(c_X[:,latent:]) + 1e-6\n",
    "        \n",
    "        # reparameterization trick\n",
    "        normal = Normal(loc=self.means,scale=self.std)\n",
    "        self.Z = normal.sample()\n",
    "        \n",
    "        \n",
    "        # decoder\n",
    "        self.decoder_layers = []\n",
    "        previous = latent\n",
    "        for current in reversed(n_list[:-1]):\n",
    "            h = DenseLayer(previous,current)\n",
    "            self.decoder_layers.append(h)\n",
    "            previous = current\n",
    "        decoder_output = DenseLayer(previous,n_input,activation=lambda x:x)\n",
    "        self.decoder_layers.append(decoder_output)\n",
    "\n",
    "        #feed forward through decoder\n",
    "        c_X = self.Z\n",
    "        for layer in self.decoder_layers:\n",
    "            c_X = layer.feed_forward(c_X)\n",
    "        logits = c_X\n",
    "        # ???\n",
    "        post_pred_logits = logits\n",
    "        \n",
    "        # output\n",
    "        self.pred = Bernoulli(logits=logits)\n",
    "        \n",
    "        # sample from output\n",
    "        self.post_pred = self.pred.sample()\n",
    "        self.post_pred_probs = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        scale = np.ones(latent,dtype=np.float32)\n",
    "        loc = scale*0\n",
    "        std_norm = Normal(loc,scale)\n",
    "        \n",
    "        Z_std = std_norm.sample(1)\n",
    "        c_X = Z_std\n",
    "        for layer in layer.decoder_layers:\n",
    "            c_X = layer.feed_forward(c_X)\n",
    "        logits = c_X\n",
    "        \n",
    "        prior_pred_dist = Bernoulli(logits=logits)\n",
    "        self.prior_pred = prior_pred_dist.sample()\n",
    "        self.prior_pred_pros = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        self.Z_input = tf.placeholder(ft.float32, shape=(None, latent))\n",
    "        c_X = self.Z_input\n",
    "        for layer in self.decoder_layers:\n",
    "            c_X = layer.feed_forward(c_X)\n",
    "        logits = c_X\n",
    "        self.prior_pred_from_in_probs = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        # cost\n",
    "        # TODO\n",
    "        self.elbo = tf.reduce_sum(expected_log_likelihood - kl)\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(-self.elbo)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.session = tf.session()\n",
    "        self.session.run(self.init)\n",
    "        \n",
    "    def fit(self,X,epochs=10,batch=50):\n",
    "        costs = []\n",
    "        n_batches = len(X) // batch\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(X)\n",
    "            for b in range(n_batches):\n",
    "                c_batch = X[b*batch:(b+1)*batch]\n",
    "                _,c, = self.session.run((self.train_op, self.elbo),feed_dict={self.X: c_batch})\n",
    "                costs.append(c)\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return self.session.run(self.means,feed_dict={self.X: X})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
