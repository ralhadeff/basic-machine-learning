{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "### Snake - Temporal difference\n",
    "Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake import Game\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import animation\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified dictionary that incorporates random defaults and epsilon greedy\n",
    "class Policy(dict):\n",
    "  \n",
    "    def __init__(self,actions,epsilon=0.1):\n",
    "        '''\n",
    "        A policy with built in epsilon greedy and a uniform random choice for non initialized keys \n",
    "        actions - list of all possible actions\n",
    "        epsilon - chance for explore\n",
    "        '''\n",
    "        self.actions = actions\n",
    "        self.eps = epsilon\n",
    "        super().__init__(self)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        c = random.random()\n",
    "        if (c<self.eps or key not in self):\n",
    "            return self.actions[random.randint(0,len(self.actions)-1)]\n",
    "        else:\n",
    "            return super().__getitem__(key)\n",
    "        \n",
    "    def update_with_Q(self,s,qs):\n",
    "        '''Update policy for a state given the Q values for this state and all available actions'''\n",
    "        # update policy for this state\n",
    "        a = np.argmax([i[1] for i in qs.items()])\n",
    "        self[s] = [i[0] for i in qs.items()][a]\n",
    "        \n",
    "    def get_definite(self, key):\n",
    "        '''\n",
    "        Get the policy without any chance of exploring\n",
    "            only missing values return the default of random choice\n",
    "        '''\n",
    "        if (key not in self):\n",
    "            return self.actions[random.randint(0,len(self.actions)-1)]\n",
    "        return super().__getitem__(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game parameters\n",
    "grid = 8\n",
    "s_size = 3\n",
    "# start game\n",
    "game = Game(grid, s_size)\n",
    "game.reset()\n",
    "\n",
    "# discount factor\n",
    "g = 0.9\n",
    "# learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# initialize policy\n",
    "policy = Policy([0,1,2,3])\n",
    "\n",
    "# dummy state for a lost game\n",
    "LOSS = tuple(np.ones(6)*-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 0\n",
      "1000 40 4\n",
      "2000 7 1\n",
      "3000 9 0\n",
      "4000 14 2\n",
      "5000 26 5\n",
      "6000 20 2\n",
      "7000 22 3\n",
      "8000 35 4\n",
      "9000 3 0\n"
     ]
    }
   ],
   "source": [
    "# Q values\n",
    "Q = {}\n",
    "Q[LOSS] = {}\n",
    "Q[LOSS][0] = 0\n",
    "# state/action counts (for decaying learning rate)\n",
    "counts = {}\n",
    "\n",
    "# run MC loop\n",
    "for i in range(10000):\n",
    "    # reset some variables\n",
    "    score = 0\n",
    "    turns = 0\n",
    "    # play one episode\n",
    "    while (True):\n",
    "        s = tuple(game.get_state())\n",
    "        a = policy[s]\n",
    "        # count turns (success in terms of not dying)\n",
    "        turns+=1\n",
    "        # count score\n",
    "        if (game.score>score):\n",
    "            score = game.score\n",
    "        # apply action\n",
    "        game.snake.d = a\n",
    "        # run one game step, get reward and check if episode is finished\n",
    "        r,e = game.iterate()\n",
    "        # decaying learning rate\n",
    "        if (s not in counts):\n",
    "            counts[s] = {}\n",
    "        if (a not in counts[s]):\n",
    "            counts[s][a] = 0\n",
    "        counts[s][a]+=1\n",
    "        lr = learning_rate/(1+counts[s][a]//10)\n",
    "        # add default Q if missing\n",
    "        if (s not in Q):\n",
    "            Q[s] = {}\n",
    "        if (a not in Q[s]):\n",
    "            Q[s][a] = 0\n",
    "        # treat specific cases\n",
    "        if (e==-1):\n",
    "            # game is lost, update with final dummy state\n",
    "            Q[s][a] = Q[s][a] + lr*(r + g*Q[LOSS][0] - Q[s][a])\n",
    "            policy.update_with_Q(s,Q[s])\n",
    "            # start a new episode\n",
    "            break\n",
    "        elif (e==1):\n",
    "            # eat apple, update modified state\n",
    "            eat = np.array(s)\n",
    "            eat[-2:] = 0\n",
    "            if (tuple(eat) not in Q):\n",
    "                Q[tuple(eat)] = {}\n",
    "                Q[tuple(eat)][0] = 0\n",
    "            Q[s][a] = Q[s][a] + lr*(r + g*Q[tuple(eat)][0] - Q[s][a])\n",
    "            policy.update_with_Q(s,Q[s])\n",
    "            r = 0\n",
    "            s = tuple(game.get_state())\n",
    "            a = policy[s]\n",
    "        else:\n",
    "            # next state and next action\n",
    "            s_p = tuple(game.get_state())\n",
    "            a_p = policy.get_definite(s_p)\n",
    "            # update Q\n",
    "            if (s_p not in Q):\n",
    "                Q[s_p] = {}\n",
    "            if (a_p not in Q[s_p]):\n",
    "                Q[s_p][a_p] = 0\n",
    "            Q[s][a] = Q[s][a] + lr*(r + g*Q[s_p][a_p] - Q[s][a])\n",
    "            # update policy for this state\n",
    "            policy.update_with_Q(s,Q[s])\n",
    "    # print progress\n",
    "    if (i%1000==0):\n",
    "        print(i,turns,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# render a game animation\n",
    "fig, ax = plt.subplots()\n",
    "image = ax.imshow(game.board)\n",
    "\n",
    "g = Game(grid, s_size)\n",
    "g.reset()\n",
    "\n",
    "def init():\n",
    "    image.set_data(g.board)\n",
    "    return (image,)\n",
    "\n",
    "def animate(i):\n",
    "    s = g.get_state()\n",
    "    a = policy.get_definite(tuple(s))\n",
    "    g.snake.d = a\n",
    "    g.iterate() \n",
    "    g.draw()\n",
    "    image.set_data(g.board)\n",
    "    return (image,)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=300, interval=120, \n",
    "                               blit=True)\n",
    "\n",
    "anim.save('./animations/q_learning.gif', writer='imagemagick', fps=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./animations/q_learning.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./animations/q_learning.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
