{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word 2 vector (word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstracts of papers about GPCRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pubmed IDs\n",
    "keyword = 'gpcr'\n",
    "max_ids = 1000\n",
    "\n",
    "url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&'\n",
    "url+= f'term={keyword}&retmax={max_ids}'\n",
    "response = requests.get(url) \n",
    "xml = response.text\n",
    "xml = xml.split('\\n')\n",
    "ids = [int(re.match('.*<Id>(\\d*)<',x)[1]) for x in xml if '<Id>' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "973"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all abstracts\n",
    "size = 10\n",
    "data = []\n",
    "for i in range(0,len(ids),size):\n",
    "    j = i+size\n",
    "    url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&'\n",
    "    url+= 'id='+','.join([str(num) for num in ids[i:j]])+'&retmode=text&rettype=abstract'\n",
    "    # split between items (two empty lines between items)\n",
    "    # and split between segments (one empty line)\n",
    "    items = [item.split('\\n\\n') for item in requests.get(url).text.split('\\n\\n\\n')]\n",
    "    # abstract should be the 4th, but sometimes it isn't   \n",
    "    data.extend([item[4] for item in items if len(item)>5])\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file\n",
    "data_processed = []\n",
    "# read line by line\n",
    "for l in data:\n",
    "    # split paragraph into discrete sentences\n",
    "    sentences = re.split('\\. |\\? |! ',l)\n",
    "    for s in sentences:\n",
    "        if (s[0].islower()):\n",
    "            # this is a wrong split (i.e. / e.g. etc), add to the end of the previous sentence\n",
    "            if (len(data_processed)>0):\n",
    "                s = data_processed.pop()+' '+s\n",
    "        # make lower case and remove all punctuation (except hyphen) and numbers\n",
    "        s = s.lower()\n",
    "        s = s.translate(str.maketrans('','',(string.punctuation+string.digits+'\\n'+'\\t').replace('-','')))\n",
    "        # add to data sentences that have at least 2 words\n",
    "        if (len(s.split())>1):\n",
    "            data_processed.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of unique words\n",
    "tokens = {}\n",
    "for s in data_processed:\n",
    "    for w in s.split():\n",
    "        tokens[w] = tokens.get(w,0)+1\n",
    "# prune out rare words\n",
    "tokens = {t for t in tokens if tokens[t]>4}\n",
    "# generate dictionary to convert tokens to ID's\n",
    "id_to_token = {i:token for (i,token) in enumerate(tokens)}\n",
    "token_to_id = {token:i for (i,token) in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pairs of target,context\n",
    "window = 5\n",
    "pairs = []\n",
    "for l in data_processed:\n",
    "    s = l.split()\n",
    "    for i in range(len(s)):\n",
    "        target = s[i]\n",
    "        if (target in token_to_id):\n",
    "            for j in range(i-window,i+window+1):\n",
    "                if (j>=0 and j!=i and j<len(s)):\n",
    "                    if (s[j] in token_to_id):\n",
    "                        pairs.append((token_to_id[target],token_to_id[s[j]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't use one-hot-encoded matrix for all the data because it is too big. Use generator instead\n",
    "def one_hot_generator(pairs, batch_size, n_words, max_iterations=None):\n",
    "    # current index\n",
    "    current = 0\n",
    "    # current iteration on all data\n",
    "    iteration = 0\n",
    "    # default is endless looping\n",
    "    if (max_iterations is None):\n",
    "        max_iterations = float('inf')\n",
    "    while (iteration<max_iterations):\n",
    "        # take batch pairs\n",
    "        batch = pairs[current:current+batch_size]\n",
    "        # one hot encode X and y\n",
    "        X = np.zeros(shape=(len(batch),n_words))\n",
    "        y = np.zeros(shape=(len(batch),n_words))\n",
    "        for i in range(len(X)):\n",
    "            X[i,batch[i][0]] = 1\n",
    "            y[i,batch[i][1]] = 1\n",
    "        # update index for next bath\n",
    "        current+=batch_size\n",
    "        yield X,y\n",
    "        # rollback to 0 at the end of the dataset\n",
    "        if (current>=len(pairs)):\n",
    "            current=0\n",
    "            iteration+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/raphael/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# size of word vector\n",
    "embed = 100\n",
    "\n",
    "input_layer = Input(shape=(len(tokens),))\n",
    "embedding = Dense(embed,use_bias=False)(input_layer)\n",
    "output = Dense(len(tokens),activation='softmax')(embedding)\n",
    "\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/raphael/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "10258/10258 [==============================] - 316s 31ms/step - loss: 6.6312\n",
      "Epoch 2/5\n",
      "10258/10258 [==============================] - 313s 31ms/step - loss: 6.3631\n",
      "Epoch 3/5\n",
      "10258/10258 [==============================] - 306s 30ms/step - loss: 6.2648\n",
      "Epoch 4/5\n",
      "10258/10258 [==============================] - 331s 32ms/step - loss: 6.1953\n",
      "Epoch 5/5\n",
      "10258/10258 [==============================] - 521s 51ms/step - loss: 6.1400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb88f5c1cf8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "model.fit_generator(one_hot_generator(pairs,batch_size,len(tokens)),steps_per_epoch=len(pairs)//batch_size+1,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belong\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['comprising', 'coupledreceptors', 'relaxin', 'belonging', 'belongs']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check one word at random\n",
    "word = np.random.randint(len(tokens))\n",
    "print(id_to_token[word])\n",
    "\n",
    "# get word vector\n",
    "word_vec = model.layers[1].get_weights()[0][word]\n",
    "# precalculate length\n",
    "b = np.sqrt(word_vec.dot(word_vec))\n",
    "\n",
    "find = 5\n",
    "# find the closest vectors to this one\n",
    "close = [(-float('inf'),-1)]\n",
    "i = 0\n",
    "for w in model.layers[1].get_weights()[0]:\n",
    "    current = (w.dot(word_vec)) / (np.sqrt(w.dot(w))*b)\n",
    "    if (current>close[0][0]):\n",
    "        if (i != word):\n",
    "            # add new element in order\n",
    "            j = 0\n",
    "            while (j<len(close) and current > close[j][0]):\n",
    "                j+=1\n",
    "            close.insert(j,(current,i))\n",
    "            # remove one element if needed\n",
    "            if (len(close)>find):\n",
    "                close.pop(0)         \n",
    "    i+=1\n",
    "\n",
    "[id_to_token[w[1]] for w in close]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairly accurate,<br>Note: the abstracts use scientific language, which is not particularly regular and natural. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
