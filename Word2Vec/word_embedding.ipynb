{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word 2 vector (word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstracts of papers about GPCRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pubmed IDs\n",
    "keyword = 'gpcr'\n",
    "max_ids = 1000\n",
    "\n",
    "url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&'\n",
    "url+= f'term={keyword}&retmax={max_ids}'\n",
    "response = requests.get(url) \n",
    "xml = response.text\n",
    "xml = xml.split('\\n')\n",
    "ids = [int(re.match('.*<Id>(\\d*)<',x)[1]) for x in xml if '<Id>' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "973"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all abstracts\n",
    "size = 10\n",
    "data = []\n",
    "for i in range(0,len(ids),size):\n",
    "    j = i+size\n",
    "    url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&'\n",
    "    url+= 'id='+','.join([str(num) for num in ids[i:j]])+'&retmode=text&rettype=abstract'\n",
    "    # split between items (two empty lines between items)\n",
    "    # and split between segments (one empty line)\n",
    "    items = [item.split('\\n\\n') for item in requests.get(url).text.split('\\n\\n\\n')]\n",
    "    # abstract should be the 4th, but sometimes it isn't   \n",
    "    data.extend([item[4] for item in items if len(item)>5])\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file\n",
    "data_processed = []\n",
    "# read line by line\n",
    "for l in data:\n",
    "    # split paragraph into discrete sentences\n",
    "    sentences = re.split('\\. |\\? |! ',l)\n",
    "    for s in sentences:\n",
    "        if (s[0].islower()):\n",
    "            # this is a wrong split (i.e. / e.g. etc), add to the end of the previous sentence\n",
    "            if (len(data_processed)>0):\n",
    "                s = data_processed.pop()+' '+s\n",
    "        # make lower case and remove all punctuation (except hyphen) and numbers\n",
    "        s = s.lower()\n",
    "        s = s.translate(str.maketrans('','',(string.punctuation+string.digits+'\\n'+'\\t').replace('-','')))\n",
    "        # add to data sentences that have at least 2 words\n",
    "        if (len(s.split())>1):\n",
    "            data_processed.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of unique words\n",
    "tokens = {}\n",
    "for s in data_processed:\n",
    "    for w in s.split():\n",
    "        tokens[w] = tokens.get(w,0)+1\n",
    "# transform to list, pruning out rare words\n",
    "    # otherwise the matrix was too large for my machine to handle\n",
    "tokens = {i for i in tokens if tokens[i]>10}\n",
    "# generate dictionary to convert tokens to ID's\n",
    "id_to_token = {i:token for (i,token) in enumerate(tokens)}\n",
    "token_to_id = {token:i for (i,token) in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pairs of target,context\n",
    "window = 5\n",
    "pairs = []\n",
    "for l in data_processed:\n",
    "    s = l.split()\n",
    "    for i in range(len(s)):\n",
    "        target = s[i]\n",
    "        if (target in token_to_id):\n",
    "            for j in range(i-window,i+window+1):\n",
    "                if (j>=0 and j!=i and j<len(s)):\n",
    "                    if (s[j] in token_to_id):\n",
    "                        pairs.append((token_to_id[target],token_to_id[s[j]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to one-hot-encoded\n",
    "n_words = len(tokens)\n",
    "X = np.zeros(shape=(len(pairs),n_words))\n",
    "y = np.zeros(shape=(len(pairs),n_words))\n",
    "for i in range(len(X)):\n",
    "    X[i,pairs[i][0]] = 1\n",
    "    y[i,pairs[i][1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of word vector\n",
    "embed = 50\n",
    "\n",
    "input_layer = Input(shape=(n_words,))\n",
    "embedding = Dense(embed,use_bias=False)(input_layer)\n",
    "output = Dense(n_words,activation='softmax')(embedding)\n",
    "\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "853932/853932 [==============================] - 118s 138us/step - loss: 5.9478\n",
      "Epoch 2/5\n",
      "853932/853932 [==============================] - 114s 134us/step - loss: 5.8189\n",
      "Epoch 3/5\n",
      "853932/853932 [==============================] - 112s 131us/step - loss: 5.7431\n",
      "Epoch 4/5\n",
      "853932/853932 [==============================] - 114s 134us/step - loss: 5.6987\n",
      "Epoch 5/5\n",
      "853932/853932 [==============================] - 113s 132us/step - loss: 5.6686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f7001dcf8>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,batch_size=100,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['specifically', 'find', 'discovered', 'detected', 'observed']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check one word at random\n",
    "word = np.random.randint(len(tokens))\n",
    "print(id_to_token[word])\n",
    "\n",
    "# get word vector\n",
    "word_vec = model.layers[1].get_weights()[0][word]\n",
    "# precalculate length\n",
    "b = np.sqrt(word_vec.dot(word_vec))\n",
    "\n",
    "find = 5\n",
    "# find the closest vectors to this one\n",
    "close = [(-float('inf'),-1)]\n",
    "i = 0\n",
    "for w in model.layers[1].get_weights()[0]:\n",
    "    current = (w.dot(word_vec)) / (np.sqrt(w.dot(w))*b)\n",
    "    if (current>close[0][0]):\n",
    "        if (i != word):\n",
    "            # add new element in order\n",
    "            j = 0\n",
    "            while (j<len(close) and current > close[j][0]):\n",
    "                j+=1\n",
    "            close.insert(j,(current,i))\n",
    "            # remove one element if needed\n",
    "            if (len(close)>find):\n",
    "                close.pop(0)         \n",
    "    i+=1\n",
    "\n",
    "[id_to_token[w[1]] for w in close]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairly accurate<br>Note: the abstracts use scientific language, where past tense and the use of words such as above are very common"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
