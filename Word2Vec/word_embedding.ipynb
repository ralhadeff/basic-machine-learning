{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word 2 vector (word embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data used is paragraphs (lines) from one of my papers, after removing all the headers, figures, and references.<br>\n",
    "https://doi.org/10.1073/pnas.1810316115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file\n",
    "raw_data = open(\"sample.txt\", \"r\")\n",
    "data = []\n",
    "# read line by line\n",
    "for l in raw_data:\n",
    "    # split paragraph into discrete sentences\n",
    "    sentences = re.split('\\. |\\? |! ',l)\n",
    "    for s in sentences:\n",
    "        if (s[0].islower()):\n",
    "            # this is a wrong split (i.e. / e.g. etc), add to the end of the previous sentence\n",
    "            if (len(data)>0):\n",
    "                s = data.pop()+' '+s\n",
    "        # make lower case and remove all punctuation (except hyphen) and numbers\n",
    "        s = s.lower()\n",
    "        s = s.translate(str.maketrans('','',(string.punctuation+string.digits+'\\n'+'\\t').replace('-','')))\n",
    "        # add to data sentences that have at least 2 words\n",
    "        if (len(s.split())>1):\n",
    "            data.append(s)\n",
    "\n",
    "# note - there are some specific issues with Fig. S# and words that are always capitalized, such as ATP\n",
    "# but overall, this should be good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of unique words\n",
    "tokens = set()\n",
    "for s in data:\n",
    "    tokens.update(s.split())\n",
    "# transform to list\n",
    "tokens = list(tokens)\n",
    "# generate dictionary to convert tokens to ID's\n",
    "token_to_id = {token:i for (i,token) in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pairs of target,context\n",
    "window = 5\n",
    "pairs = []\n",
    "for l in data:\n",
    "    s = l.split()\n",
    "    for i in range(len(s)):\n",
    "        target = s[i]\n",
    "        for j in range(i-window,i+window+1):\n",
    "            if (j>=0 and j!=i and j<len(s)):\n",
    "                pairs.append((token_to_id[target],token_to_id[s[j]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to one-hot-encoded\n",
    "n_words = len(tokens)\n",
    "X = np.zeros(shape=(len(pairs),n_words))\n",
    "y = np.zeros(shape=(len(pairs),n_words))\n",
    "for i in range(len(X)):\n",
    "    X[i,pairs[i][0]] = 1\n",
    "    y[i,pairs[i][1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of word vector\n",
    "embed = 20\n",
    "\n",
    "input_layer = Input(shape=(n_words,))\n",
    "embedding = Dense(embed,use_bias=False)(input_layer)\n",
    "output = Dense(n_words,activation='softmax')(embedding)\n",
    "\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "44010/44010 [==============================] - 7s 157us/step - loss: 6.2283\n",
      "Epoch 2/20\n",
      "44010/44010 [==============================] - 6s 144us/step - loss: 5.7359\n",
      "Epoch 3/20\n",
      "44010/44010 [==============================] - 7s 149us/step - loss: 5.6876\n",
      "Epoch 4/20\n",
      "44010/44010 [==============================] - 7s 151us/step - loss: 5.6641\n",
      "Epoch 5/20\n",
      "44010/44010 [==============================] - 7s 149us/step - loss: 5.6331\n",
      "Epoch 6/20\n",
      "44010/44010 [==============================] - 6s 147us/step - loss: 5.5872\n",
      "Epoch 7/20\n",
      "44010/44010 [==============================] - 7s 150us/step - loss: 5.5254\n",
      "Epoch 8/20\n",
      "44010/44010 [==============================] - 6s 143us/step - loss: 5.4506\n",
      "Epoch 9/20\n",
      "44010/44010 [==============================] - 6s 146us/step - loss: 5.3708\n",
      "Epoch 10/20\n",
      "44010/44010 [==============================] - 6s 147us/step - loss: 5.2912\n",
      "Epoch 11/20\n",
      "44010/44010 [==============================] - 6s 144us/step - loss: 5.2159\n",
      "Epoch 12/20\n",
      "44010/44010 [==============================] - 6s 140us/step - loss: 5.1460\n",
      "Epoch 13/20\n",
      "44010/44010 [==============================] - 6s 141us/step - loss: 5.0819\n",
      "Epoch 14/20\n",
      "44010/44010 [==============================] - 7s 152us/step - loss: 5.0243\n",
      "Epoch 15/20\n",
      "44010/44010 [==============================] - 7s 149us/step - loss: 4.9729\n",
      "Epoch 16/20\n",
      "44010/44010 [==============================] - 6s 146us/step - loss: 4.9273\n",
      "Epoch 17/20\n",
      "44010/44010 [==============================] - 6s 141us/step - loss: 4.8874\n",
      "Epoch 18/20\n",
      "44010/44010 [==============================] - 6s 145us/step - loss: 4.8515\n",
      "Epoch 19/20\n",
      "44010/44010 [==============================] - 6s 142us/step - loss: 4.8205\n",
      "Epoch 20/20\n",
      "44010/44010 [==============================] - 6s 147us/step - loss: 4.7933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5ff6fe748>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,batch_size=25,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enzyme\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'catalysis'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check one word at random\n",
    "word = np.random.randint(len(tokens))\n",
    "print(tokens[word])\n",
    "\n",
    "# get word vector\n",
    "word_vec = model.layers[1].get_weights()[0][word]\n",
    "# precalculate length\n",
    "b = np.sqrt(word_vec.dot(word_vec))\n",
    "\n",
    "# find the closest vector to this one\n",
    "cos = -float('inf')\n",
    "idx = -1\n",
    "i = 0\n",
    "for w in model.layers[1].get_weights()[0]:\n",
    "    current = (w.dot(word_vec)) / (np.sqrt(w.dot(w))*b)\n",
    "    if (current>cos):\n",
    "        if (i != word):\n",
    "            cos = current\n",
    "            idx = i\n",
    "    i+=1\n",
    "\n",
    "tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'performs'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try some arithmetric\n",
    "w1, w2 = token_to_id['energy'], token_to_id['simulation']\n",
    "vec = model.layers[1].get_weights()[0][w1] + model.layers[1].get_weights()[0][w2]\n",
    "# precalculate length\n",
    "b = np.sqrt(vec.dot(vec))\n",
    "\n",
    "# find the closest vector to this one\n",
    "cos = -float('inf')\n",
    "idx = -1\n",
    "i = 0\n",
    "for w in model.layers[1].get_weights()[0]:\n",
    "    current = (w.dot(vec)) / (np.sqrt(w.dot(w))*b)\n",
    "    if (current>cos):\n",
    "        if (i != w1 and i!= w2):\n",
    "            cos = current\n",
    "            idx = i\n",
    "    i+=1\n",
    "\n",
    "tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpcr-mediated',\n",
       " 'look',\n",
       " 'conditions',\n",
       " 'selected',\n",
       " 'us',\n",
       " 'estimated',\n",
       " 'projection',\n",
       " 'performs',\n",
       " 'fairly']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try some arithmetric\n",
    "w1, w2 = token_to_id['protein'], token_to_id['energy']\n",
    "vec = model.layers[1].get_weights()[0][w1] + model.layers[1].get_weights()[0][w2]\n",
    "# precalculate length\n",
    "b = np.sqrt(vec.dot(vec))\n",
    "\n",
    "# find the closest vector to this one\n",
    "cos = -float('inf')\n",
    "idx = []\n",
    "i = 0\n",
    "for w in model.layers[1].get_weights()[0]:\n",
    "    current = (w.dot(vec)) / (np.sqrt(w.dot(w))*b)\n",
    "    if (current>cos):\n",
    "        if (i != w1 and i!= w2):\n",
    "            cos = current\n",
    "            idx.append((i,cos))\n",
    "    i+=1\n",
    "\n",
    "[tokens[i[0]] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
